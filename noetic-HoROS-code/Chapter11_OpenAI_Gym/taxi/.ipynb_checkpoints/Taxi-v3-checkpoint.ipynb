{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the taxi environment and render it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "The problem we are going to solve consists of a self-driving cab that has to pick up passengers and drop off to the right location, while doing as quick as possible and respecting the traffic rules.\n",
    "\n",
    "There are 4 locations (R, G, B, Y) and the taxi hast to pick up the passenger at one location and drop him off at another.\n",
    "- The agent receives +20 points for a successful dropoff\n",
    "- It loses -1 point for every time-step. This way we encourage him to solve the enviroment as fast as possible: all the time it is on the road is consuming resources, such as fuel, so this negative reward may also have sense for this aspect.\n",
    "- It is given a -10 points penalty for every illegal action it performs (in the pickup or dropoff actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "Reset the environment to a initial random state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action space\n",
    "There are 6 possible actions that are encoded as follows:\n",
    "- 0 = south\n",
    "- 1 = north\n",
    "- 2 = east\n",
    "- 3 = west\n",
    "- 4 = pickup\n",
    "- 5 = dropoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State space\n",
    "Taxi environment has 500 total possible states =\n",
    "  - 5 rows ×\n",
    "  - 5 cols ×\n",
    "  - 5 (4 static +1 passenger inside taxi(4) locations) × [R(0), G(-), Y(-), B(2)]\n",
    "  - 4 destinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the value corresponding to an encoded state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('State:', 328)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can also manually set the state\n",
    "env.s = 328\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is defined as a Python dictionary  **P** that has as many entries  as possible actions (6). The default reward values are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entries of the dictionary has this structure:\n",
    "\n",
    "`{action: [(probability, nextstate, reward, done)]}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The key is the action number as encoded above\n",
    "- We set their probabilities to 1.0. This way all actions are equally possible when the agent hast to take a random decission.\n",
    "- All the movement actions provide a penalty of -1 to account for the time it runs before the passenger is drop off at the destination\n",
    "- Both the pickup and drop off actions have -10 penalty\n",
    "- If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)\n",
    "- `done` signs a successful passenger drop off (in the right location). This event marks the succesful completion of an **episode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution using brute force\n",
    "Iterate until one passenger reaches one destination (the agent completes one episode) = received reward is 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 221\n",
      "Penalties incurred: 52\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "Timestep: 1\n",
      "State: 261\n",
      "Action: 1\n",
      "Reward: -1\n"
     ]
    }
   ],
   "source": [
    "# This snippet replays all the steps of the brute force solution above \n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames, delay = 0.1):\n",
    "    for i, frame in enumerate(frames):\n",
    "        if i==1:\n",
    "            break\n",
    "        clear_output(wait=True)\n",
    "        print frame['frame']\n",
    "        print \"Timestep: {}\".format(i + 1)\n",
    "        print \"State: {}\".format(frame['state'])\n",
    "        print \"Action: {}\".format(frame['action'])\n",
    "        print \"Reward: {}\".format(frame['reward'])\n",
    "        sleep(delay)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent may take hundreds or even thousands of timesteps and makes lots of wrong drop offs in front of only one sucessful delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Reinforcement Learning\n",
    "Q-learning algorithm lets the agent keep track of rewards to learn the best action for every single state.\n",
    "- Each time an **action** is taken from a given **state**, a **reward** is obtained according to P\n",
    "- The **reward** associated to each pair **(state, action)** creates a Q-table that is a 500 x 6 matrix \n",
    "\n",
    "The Q-value for a concrete pair state-action stand for the \"quality\" of that action in that state. Hence for a completely trained model, we will have a full 500 x 6 matrix, i.e., 3000 Q-values.\n",
    "- Every row represents an state\n",
    "- The maximum Q-value in such row let the agent know what action is the best to take in that state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:\n",
    "\n",
    "`Q(state,action) ← (1−α)Q(state,action) + α(reward + γ maxQ(next state,all actions))`\n",
    "- **α** is the learning rate (0<α≤1)\n",
    "- **γ** is the discount factor (0≤γ≤1) and determines how much importance we want to give to future rewards.\n",
    "   -  If this factor is 0 makes the agent to consider only the immediate reward, making it to behave in a **greedy** manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trade-off arises between **exploration** (= taking a random action) and **exploitation** (using learned Q-values to decide the action).\n",
    "We need to balance these two behaviors, and for that we introduce the parameter **ϵ \"epsilon\"** that represent the percentage of actions that should be of the  **exploration** type.\n",
    "This prevents the agent to follow a single route (because it only knows a limited set of rewards, those of the states it visits = **overfitting**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Q-table to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning rate: `α = 0.1`\n",
    "- Discount factor: `γ = 0.6` (don't be greedy, be patient and postpone the reward)\n",
    "- Exploration rate: `ϵ = 0.1` (1-ϵ = 90% is explotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n",
      "CPU times: user 26.9 s, sys: 79.7 ms, total: 27 s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    frames = [] # for animation\n",
    "    \n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    n_steps = 0 # Index for counting how many steps in each episode\n",
    "    while not done:\n",
    "        n_steps += 1\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 1000 == 0:\n",
    "        sleep(1)\n",
    "        print \"Episode: {} Reward= {}, Number of steps= {}\".format(i, reward, n_steps)\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "print(\"Training finished.\\n\")\n",
    "#print frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Q-table has been established over 100,000 episodes, let's see what the Q-values are at our illustration's state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.40437772,  -2.27325184,  -2.40857979,  -2.35569503,\n",
       "       -10.7548625 , -10.86157945])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max Q-value is \"north\" (-2.273) and this is the action to be taken from state 328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "Timestep: 1\n",
      "State: 367\n",
      "Action: 1\n",
      "Reward: -1\n",
      "This was the replay of the LAST episode: \n",
      "\n",
      "Results after 100 episodes:\n",
      " - Average timesteps per episode: 12\n",
      " - Average penalties per episode: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    frames = [] # for animation\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        \n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            })\n",
    "        \n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print_frames(frames, 1)\n",
    "print \"This was the replay of the LAST episode: \\n\"\n",
    "\n",
    "print \"Results after {} episodes:\".format(episodes)\n",
    "print \" - Average timesteps per episode: {}\".format(total_epochs / episodes)\n",
    "print \" - Average penalties per episode: {}\".format(total_penalties / episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "How to chose  the values of `alpha`, `gamma`, and `epsilon`? It will be mainly based on both intuition and hit and trial. In any case,  the three of them should decrease over time as the agent learns the best actions:\n",
    "- decreasing the need of learning (`alpha`)\n",
    "- As an optimum strategy is learned, there is no need to weight further actions in the next state. The agent knows well what will happen (`gamma`)\n",
    "- When the environment is fully explored, random actions  have no sense (`epsilon`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
